<!doctype html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060 - Exploring Azure, DevOps and Software Development</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Exploring Azure, DevOps and Software Development"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Exploring Azure, DevOps and Software Development"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="üéØ TL;DR: Subject-Driven Image Generation on 12GB VRAM Large AI models like FLUX.1-schnell typically require datacenter GPUs with 48GB+ VRAM. Problem: Most developers and hobbyists only have access"><meta property="og:type" content="blog"><meta property="og:title" content="Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060"><meta property="og:url" content="https://clouddev.blog/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/"><meta property="og:site_name" content="Exploring Azure, DevOps and Software Development"><meta property="og:description" content="üéØ TL;DR: Subject-Driven Image Generation on 12GB VRAM Large AI models like FLUX.1-schnell typically require datacenter GPUs with 48GB+ VRAM. Problem: Most developers and hobbyists only have access"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://clouddev.blog/img/running-llms-consumer-gpu.png"><meta property="article:published_time" content="2025-11-10T11:00:00.000Z"><meta property="article:modified_time" content="2026-01-07T04:21:18.650Z"><meta property="article:author" content="Ricky Gummadi"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Local GPUs"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://clouddev.blog/img/running-llms-consumer-gpu.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://clouddev.blog/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/"},"headline":"Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060","image":["https://clouddev.blog/img/running-llms-consumer-gpu.png"],"datePublished":"2025-11-10T11:00:00.000Z","dateModified":"2026-01-07T04:21:18.650Z","author":{"@type":"Person","name":"Ricky Gummadi"},"publisher":{"@type":"Organization","name":"Exploring Azure, DevOps and Software Development","logo":{"@type":"ImageObject","url":"https://clouddev.blog/img/logo.png"}},"description":"üéØ TL;DR: Subject-Driven Image Generation on 12GB VRAM Large AI models like FLUX.1-schnell typically require datacenter GPUs with 48GB+ VRAM. Problem: Most developers and hobbyists only have access"}</script><link rel="canonical" href="https://clouddev.blog/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/"><link rel="alternate" href="/atom.xml" title="Exploring Azure, DevOps and Software Development" type="application/atom+xml"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-M9TEW2G3HK" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-M9TEW2G3HK")</script><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><script data-ad-client="ca-pub-1649927746910796" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><script>!function(){function e(){if(!location.hash)return;Array.from(document.querySelectorAll(".tab-content")).forEach(e=>{e.classList.add("is-hidden")}),Array.from(document.querySelectorAll(".tabs li")).forEach(e=>{e.classList.remove("is-active")});const e=document.querySelector(location.hash);e&&e.classList.remove("is-hidden");const t=document.querySelector(`a[href="${location.hash}"]`);t&&t.parentElement.classList.add("is-active")}e(),window.addEventListener("hashchange",e,!1)}()</script><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Exploring Azure, DevOps and Software Development" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Ricky-G/ricky-g.github.io"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/running-llms-consumer-gpu.png" alt="Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2025-11-10T11:00:00.000Z" title="11/11/2025, 12:00:00 AM">2025-11-11</time></span><span class="level-item">Updated&nbsp;<time datetime="2026-01-07T04:21:18.650Z" title="1/7/2026, 5:21:18 PM">2026-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span>¬†/¬†</span><a class="link-muted" href="/categories/AI/LLMs/">LLMs</a></span><span class="level-item">21 minutes read (About 3110 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060</h1><div class="content"><hr><blockquote><p><strong>üéØ TL;DR: Subject-Driven Image Generation on 12GB VRAM</strong></p><p>Large AI models like FLUX.1-schnell typically require datacenter GPUs with 48GB+ VRAM. Problem: Most developers and hobbyists only have access to consumer RTX cards which vary from 6 - 12GB VRAM in most cases (with the exception of the expensive 4090&#x2F;5090 cards which can go up to 32gb).</p><p><strong>Solution:</strong> Using mmgp (Memory Management for GPU Poor) with Docker containerization enables FLUX.1 OmniControl to run on RTX 3060 12GB through 8-bit quantization, dynamic VRAM&#x2F;RAM offloading, and selective layer loading. The implementation provides a Gradio web interface generating 512x512 images in ~10 seconds after initial model loading, with models persisting in system RAM to avoid reload overhead.</p><p><strong>Technical Approach:</strong> Profile 3 configuration quantizes the T5 text encoder (8.8GB ‚Üí ~4.4GB), pins the FLUX transformer (22.7GB) to reserved system RAM, and dynamically loads only active layers to VRAM during inference. Tested and validated on RTX 3060 12GB with 64GB system RAM running Windows 11 + WSL2 + Docker Desktop.</p><p><strong>Complete Implementation:</strong> All code, Dockerfile, and setup instructions are available at <a target="_blank" rel="noopener" href="https://github.com/Ricky-G/docker-ai-models/tree/main/omnicontrol">github.com&#x2F;Ricky-G&#x2F;docker-ai-models&#x2F;omnicontrol</a></p></blockquote><hr><p>Recently, I wanted to experiment with OmniControl, a subject-driven image generation model that extends FLUX.1-schnell with LoRA adapters for precise control over object placement. The challenge? The model requirements listed 48GB+ VRAM, and I only had an RTX 3060 with 12GB sitting in my workstation.</p><p>This is a common frustration in the AI development community. Research papers showcase impressive results on expensive datacenter hardware, but practical implementation on consumer GPUs requires significant engineering effort. Could I actually run this model locally without upgrading to an RTX 4090&#x2F;5090 or pay for a VM in Azure with A100?</p><p>The answer turned out to be yes - with some clever memory management and containerization. This blog post walks through the complete process of dockerizing OmniControl to run efficiently on a 12GB consumer GPU.</p><span id="more"></span><h2 id="What-is-FLUX-1-OmniControl"><a href="#What-is-FLUX-1-OmniControl" class="headerlink" title="What is FLUX.1 OmniControl?"></a>What is FLUX.1 OmniControl?</h2><p>Before diving into the technical implementation, let‚Äôs understand what we‚Äôre working with. <strong>FLUX.1 OmniControl</strong> is a subject-driven image generation model that extends the base FLUX.1-schnell diffusion model with LoRA (Low-Rank Adaptation) adapters for precise control over object placement and composition.</p><p>Unlike traditional text-to-image models where you only provide a text prompt, OmniControl allows you to:</p><ul><li><strong>Subject Consistency:</strong> Provide a reference image of a specific object (like a toy, person, or product) and have it accurately reproduced in generated images</li><li><strong>Spatial Control:</strong> Specify exactly where in the scene you want objects placed</li><li><strong>Style Preservation:</strong> Maintain the visual characteristics of the reference object across different contexts and environments</li></ul><p>Think of it as ‚ÄúPhotoshop + AI‚Äù - you can place your specific objects into any scene you can describe with text. This makes it incredibly powerful for product visualization, creative content generation, and prototyping visual concepts.</p><p>The trade-off? The model is <strong>massive</strong> - requiring over 30GB of model weights to achieve this level of control and quality. This is where the engineering challenge begins.</p><h2 id="The-Challenge-Model-Size-vs-Available-VRAM"><a href="#The-Challenge-Model-Size-vs-Available-VRAM" class="headerlink" title="The Challenge: Model Size vs Available VRAM"></a>The Challenge: Model Size vs Available VRAM</h2><p>Let‚Äôs start with the hard numbers:</p><p><strong>FLUX.1-schnell model components:</strong></p><ul><li>Transformer: 22.7GB (torch.bfloat16)</li><li>T5 Text Encoder: 8.8GB</li><li>CLIP Text Encoder: 162MB</li><li>VAE: ~1GB</li><li>OminiControl LoRA: 200MB</li><li><strong>Total</strong>: ~32.8GB of model weights</li></ul><p><strong>Available hardware:</strong><br>I am constrained by my existing workstation specs:</p><ul><li>RTX 3060: 12GB VRAM</li><li>System RAM: 64GB DDR4</li><li>Storage: 1TB NVMe SSD + 2TB HDD</li><li>CPU: Intel i7-11700K</li><li>OS: Windows 11 + WSL2 (Ubuntu 22.04)</li><li>Docker Desktop with NVIDIA Container Toolkit</li></ul><p>The gap is obvious - we need nearly 3x more VRAM than the GPU provides. Traditional approaches like FP16 precision or model pruning weren‚Äôt going to cut it. We needed something more aggressive.</p><h2 id="Understanding-mmgp-Memory-Management-for-GPU-Poor"><a href="#Understanding-mmgp-Memory-Management-for-GPU-Poor" class="headerlink" title="Understanding mmgp: Memory Management for GPU Poor"></a>Understanding mmgp: Memory Management for GPU Poor</h2><p>The key enabler for this project is <a target="_blank" rel="noopener" href="https://pypi.org/project/mmgp/">mmgp</a> (Memory Management for GPU Poor), a Python library specifically designed to run large models on consumer hardware. Here‚Äôs how it works:</p><h3 id="8-Bit-Quantization"><a href="#8-Bit-Quantization" class="headerlink" title="8-Bit Quantization"></a>8-Bit Quantization</h3><p>mmgp uses <code>quanto</code> to quantize large model components from 16-bit to 8-bit precision:</p><ul><li>T5 encoder: 8.8GB ‚Üí ~4.4GB (50% reduction)</li><li>Quality impact: Minimal for text encoding tasks</li><li>Speed impact: Slight increase in encoding time (~10-15%)</li></ul><h3 id="Dynamic-VRAM-x2F-RAM-Offloading"><a href="#Dynamic-VRAM-x2F-RAM-Offloading" class="headerlink" title="Dynamic VRAM&#x2F;RAM Offloading"></a>Dynamic VRAM&#x2F;RAM Offloading</h3><p>Instead of keeping all model weights in VRAM, mmgp maintains a ‚Äúworking set‚Äù:</p><ul><li>Critical layers: Loaded to VRAM during active use</li><li>Inactive layers: Offloaded to pinned system RAM</li><li>Transfers: Handled automatically during forward passes</li></ul><h3 id="RAM-Pinning-Strategy"><a href="#RAM-Pinning-Strategy" class="headerlink" title="RAM Pinning Strategy"></a>RAM Pinning Strategy</h3><p>Models are loaded once from disk to system RAM (one-time cost), then:</p><ul><li>Pinned memory allocation: 75% of system RAM reserved (48GB in my case)</li><li>Fast transfers: Pinned RAM ‚Üí VRAM takes ~200ms for 1GB</li><li>Persistent storage: Models stay in RAM across generations</li></ul><h3 id="Profile-System"><a href="#Profile-System" class="headerlink" title="Profile System"></a>Profile System</h3><p>mmgp provides 5 preconfigured profiles:</p><table><thead><tr><th>Profile</th><th>Target VRAM</th><th>Strategy</th><th>Use Case</th></tr></thead><tbody><tr><td>1</td><td>16-24GB</td><td>Full model in VRAM</td><td>Maximum speed</td></tr><tr><td>2</td><td>12-16GB</td><td>Partial VRAM + RAM</td><td>Balanced</td></tr><tr><td><strong>3</strong></td><td><strong>12GB</strong></td><td><strong>Quantization + pinning</strong></td><td><strong>RTX 3060 sweet spot</strong></td></tr><tr><td>4</td><td>8-12GB</td><td>Aggressive quantization</td><td>Lower-end cards</td></tr><tr><td>5</td><td>6-8GB</td><td>Minimal VRAM usage</td><td>GPU Poor mode</td></tr></tbody></table><p>For RTX 3060, Profile 3 provides the best balance between speed and stability.</p><h2 id="Prerequisites-What-You‚Äôll-Need"><a href="#Prerequisites-What-You‚Äôll-Need" class="headerlink" title="Prerequisites: What You‚Äôll Need"></a>Prerequisites: What You‚Äôll Need</h2><p>Before starting the implementation, ensure you have the following components set up:</p><h3 id="Hardware-Requirements"><a href="#Hardware-Requirements" class="headerlink" title="Hardware Requirements"></a>Hardware Requirements</h3><p><strong>Minimum Configuration:</strong></p><ul><li>NVIDIA GPU: 12GB VRAM (RTX 3060, 3060 Ti, or better)</li><li>System RAM: 64GB DDR4&#x2F;DDR5 (48GB will be pinned for model storage)</li><li>Storage: 50GB free space (35GB for models + overhead)</li><li>CPU: Any modern multi-core processor</li></ul><p><strong>Recommended Configuration:</strong></p><ul><li>GPU: RTX 3060 12GB or RTX 4060 Ti 16GB</li><li>RAM: 64GB or more</li><li>Storage: NVMe SSD for faster startup times (HDD works but adds 2-3 min to load times)</li></ul><h3 id="Software-Requirements"><a href="#Software-Requirements" class="headerlink" title="Software Requirements"></a>Software Requirements</h3><p><strong>Windows Users:</strong></p><ul><li>Windows 11 (Windows 10 with WSL2 also works)</li><li>WSL2 installed and configured</li><li>Docker Desktop for Windows (latest version)</li><li>NVIDIA Container Toolkit (installed via Docker Desktop)</li></ul><p><strong>Linux Users:</strong></p><ul><li>Ubuntu 22.04 or similar distribution</li><li>Docker Engine (latest version)</li><li>NVIDIA Container Toolkit</li><li>NVIDIA drivers (version 525+)</li></ul><h3 id="Account-Requirements"><a href="#Account-Requirements" class="headerlink" title="Account Requirements"></a>Account Requirements</h3><ul><li><strong>HuggingFace Account:</strong> Required to download models</li><li><strong>HuggingFace Token:</strong> Generate a read-access token at <a target="_blank" rel="noopener" href="https://huggingface.co/settings/tokens">huggingface.co&#x2F;settings&#x2F;tokens</a></li></ul><h3 id="Verification-Steps"><a href="#Verification-Steps" class="headerlink" title="Verification Steps"></a>Verification Steps</h3><p>Before proceeding, verify your setup:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check GPU availability</span></span><br><span class="line">nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify Docker installation</span></span><br><span class="line">docker --version</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test GPU access in Docker</span></span><br><span class="line">docker run --<span class="built_in">rm</span> --gpus all nvidia/cuda:12.1.1-base-ubuntu22.04 nvidia-smi</span><br></pre></td></tr></table></figure><p>If all commands execute successfully, you‚Äôre ready to begin!</p><h2 id="Docker-Architecture-Why-Containerization"><a href="#Docker-Architecture-Why-Containerization" class="headerlink" title="Docker Architecture: Why Containerization?"></a>Docker Architecture: Why Containerization?</h2><p>With prerequisites confirmed, let‚Äôs talk about <strong>why Docker</strong> is the right choice for this project. Running large AI models involves complex dependency chains - specific versions of PyTorch, CUDA libraries, Python packages, and system libraries that can conflict with your existing environment.</p><blockquote><p><strong>üí° Want to Skip Ahead?</strong></p><p>The complete Docker implementation, including the Dockerfile, all Python code, and deployment scripts, is available in my GitHub repository: <strong><a target="_blank" rel="noopener" href="https://github.com/Ricky-G/docker-ai-models/tree/main/omnicontrol">docker-ai-models&#x2F;omnicontrol</a></strong></p><p>You can clone and run it immediately, or continue reading to understand how it works under the hood.</p></blockquote><p>Containerization solves this by:</p><ul><li><strong>Isolating dependencies</strong> from your host system</li><li><strong>Ensuring reproducibility</strong> across different machines</li><li><strong>Simplifying deployment</strong> - one command to run the entire stack</li><li><strong>Enabling version control</strong> of the entire environment</li></ul><p>The containerization approach provides several additional benefits:</p><ul><li>Eliminates dependency conflicts</li><li>Ensures reproducible builds</li><li>Simplifies deployment across machines</li><li>Isolates model storage from application code</li></ul><h3 id="Container-Structure"><a href="#Container-Structure" class="headerlink" title="Container Structure"></a>Container Structure</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04</span><br><span class="line">‚îú‚îÄ‚îÄ Python 3.10 + CUDA libraries</span><br><span class="line">‚îú‚îÄ‚îÄ PyTorch 2.0 with CUDA support</span><br><span class="line">‚îú‚îÄ‚îÄ Diffusers + Transformers</span><br><span class="line">‚îú‚îÄ‚îÄ mmgp for memory management</span><br><span class="line">‚îú‚îÄ‚îÄ Gradio for web interface</span><br><span class="line">‚îî‚îÄ‚îÄ Custom FLUX integration code</span><br></pre></td></tr></table></figure><h3 id="Volume-Mounts"><a href="#Volume-Mounts" class="headerlink" title="Volume Mounts"></a>Volume Mounts</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-v D:\_Models\omnicontrol:/app/models    <span class="comment"># Persistent model storage (34GB)</span></span><br></pre></td></tr></table></figure><p>Models download once to the host system and persist across container rebuilds. This is critical for development iteration - rebuilding the container doesn‚Äôt trigger 30-minute model downloads.</p><h3 id="GPU-Access"><a href="#GPU-Access" class="headerlink" title="GPU Access"></a>GPU Access</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--gpus all    <span class="comment"># Exposes all NVIDIA GPUs to container</span></span><br></pre></td></tr></table></figure><p>Docker Desktop + NVIDIA Container Toolkit handles GPU passthrough automatically on Windows via WSL2.</p><h2 id="Implementation-Details-From-Code-to-Running-System"><a href="#Implementation-Details-From-Code-to-Running-System" class="headerlink" title="Implementation Details: From Code to Running System"></a>Implementation Details: From Code to Running System</h2><p>Now that we understand the architecture and tools, let‚Äôs dive into how everything works in practice. This section covers the actual startup sequence, performance characteristics, and a critical optimization that makes this entire approach viable.</p><blockquote><p><strong>‚ö° Key Performance Insight Ahead</strong></p><p>One of the biggest challenges in this implementation was preventing VRAM from being cleared after each generation, which would cause 80+ second reload times. The solution? A single line of code change that reduced subsequent generation times from 120s to 10s. We‚Äôll cover this critical fix in detail below.</p></blockquote><h3 id="Startup-Sequence"><a href="#Startup-Sequence" class="headerlink" title="Startup Sequence"></a>Startup Sequence</h3><p>The container initialization follows this sequence:</p><p><strong>1. GPU Detection</strong> (~1 second)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi --query-gpu=name,memory.total --format=csv</span><br><span class="line"><span class="comment"># Output: NVIDIA GeForce RTX 3060, 12288 MiB</span></span><br></pre></td></tr></table></figure><p><strong>2. Profile Selection</strong> (automatic)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vram = get_gpu_memory()</span><br><span class="line"><span class="keyword">if</span> vram &gt;= <span class="number">11000</span>:</span><br><span class="line">    profile = <span class="number">3</span>  <span class="comment"># 12GB optimized</span></span><br></pre></td></tr></table></figure><p><strong>3. Model Loading</strong> (2-3 minutes from HDD)</p><ul><li>FLUX.1-schnell: Downloads from HuggingFace (~22.7GB)</li><li>OminiControl LoRA: Downloads adapter weights (~200MB)</li><li>Loads to CPU first, then applies mmgp profiling</li></ul><p><strong>4. mmgp Profiling</strong> (1-2 minutes)</p><ul><li>Quantizes T5 encoder to 8-bit</li><li>Allocates 48GB pinned RAM (75% of 64GB)</li><li>Hooks model layers for dynamic offloading</li></ul><p><strong>5. Gradio Launch</strong> (~5 seconds)</p><ul><li>Web interface starts on port 7860</li><li>Ready to accept generation requests</li></ul><p><strong>Total first-run time:</strong> 5-10 minutes (mostly downloading models)<br><strong>Subsequent runs:</strong> ~3 minutes (loading from disk to RAM)</p><h3 id="Generation-Performance"><a href="#Generation-Performance" class="headerlink" title="Generation Performance"></a>Generation Performance</h3><p><strong>First generation after startup:</strong></p><ul><li>Time: ~110 seconds</li><li>Breakdown:<ul><li>VRAM loading: 80 seconds (22.7GB from RAM ‚Üí VRAM)</li><li>Actual inference: 30 seconds (8 steps @ 512x512)</li></ul></li><li>GPU memory: Climbs from 3GB ‚Üí 10-12GB</li></ul><p><strong>Subsequent generations:</strong></p><ul><li>Time: ~10 seconds (target achieved!)</li><li>VRAM stays at 10-12GB between generations</li><li>No reload overhead</li></ul><h3 id="Critical-Fix-Preventing-VRAM-Clearing"><a href="#Critical-Fix-Preventing-VRAM-Clearing" class="headerlink" title="Critical Fix: Preventing VRAM Clearing"></a>Critical Fix: Preventing VRAM Clearing</h3><blockquote><p><strong>üö® This Section Contains The Key Optimization</strong></p><p>Initial testing revealed a major performance bottleneck that would have made this entire approach impractical. Understanding and fixing this issue is critical for achieving acceptable performance.</p></blockquote><p><strong>The Problem:</strong></p><p>During initial testing, the first image generation took about 110 seconds (expected), but <strong>every subsequent generation also took 110+ seconds</strong>. Monitoring GPU memory usage revealed the issue:</p><ul><li>After generation completes: VRAM drops from 10-12GB back to 3GB</li><li>Next generation starts: 80 seconds spent reloading models from RAM to VRAM</li><li>Inference runs: 30 seconds of actual generation</li><li><strong>Total: 110 seconds per image, no matter how many you generate</strong></li></ul><p>This made the system unusable for practical work - imagine waiting nearly 2 minutes for every single image!</p><p><strong>The Root Cause:</strong></p><p>Diagnosis revealed that FLUX‚Äôs generation code was calling <code>maybe_free_model_hooks()</code> after every inference pass. This function is designed to free memory for systems running multiple models or tight memory scenarios, but in our case where we want to generate multiple images in sequence, it was counterproductive.</p><p>The culprit was in <code>src/flux/generate.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEFORE (problematic)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>():</span><br><span class="line">    <span class="comment"># ... generation code ...</span></span><br><span class="line">    self.maybe_free_model_hooks()  <span class="comment"># ‚ùå Unloads everything from VRAM!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># AFTER (fixed)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>():</span><br><span class="line">    <span class="comment"># ... generation code ...</span></span><br><span class="line">    <span class="comment"># DISABLED: Keep models in VRAM between generations</span></span><br><span class="line">    <span class="comment"># self.maybe_free_model_hooks()  # ‚úÖ Models stay loaded</span></span><br></pre></td></tr></table></figure><p><strong>The Impact:</strong></p><p>This single line change transformed the performance profile:</p><table><thead><tr><th>Metric</th><th>Before Fix</th><th>After Fix</th><th>Improvement</th></tr></thead><tbody><tr><td>First generation</td><td>110s</td><td>110s</td><td>(same)</td></tr><tr><td>Second generation</td><td>110s</td><td><strong>10s</strong></td><td><strong>11x faster</strong></td></tr><tr><td>Third generation</td><td>110s</td><td><strong>10s</strong></td><td><strong>11x faster</strong></td></tr><tr><td>VRAM after gen</td><td>3GB</td><td>10-12GB</td><td>(persistent)</td></tr></tbody></table><p>Suddenly, generating 10 images went from 18 minutes to just 2 minutes (110s + 9 √ó 10s). This made the difference between ‚Äútechnically possible but impractical‚Äù and ‚Äúactually usable for real work.‚Äù</p><p><strong>üìÇ See the Implementation:</strong></p><p>The complete modified FLUX generation code with this optimization is available in the GitHub repository at <a target="_blank" rel="noopener" href="https://github.com/Ricky-G/docker-ai-models/blob/main/omnicontrol/src/flux/generate.py"><code>src/flux/generate.py</code></a>. You can see exactly how the model loading and generation pipeline is structured, along with all the mmgp integration code.</p><h2 id="Real-World-Testing-Results"><a href="#Real-World-Testing-Results" class="headerlink" title="Real-World Testing Results"></a>Real-World Testing Results</h2><h3 id="Test-Configuration"><a href="#Test-Configuration" class="headerlink" title="Test Configuration"></a>Test Configuration</h3><p><strong>Hardware:</strong> RTX 3060 12GB, Intel i7-11700K, 64GB DDR4, Micron NVMe main drive, 7200RPM HDD secondary</p><p><strong>OS:</strong> Windows 11 + WSL2 (Ubuntu 22.04)</p><p><strong>Docker:</strong> Desktop 4.28 with NVIDIA Container Toolkit</p><p><strong>Model:</strong> FLUX.1-schnell + OminiControl subject_512.safetensors</p><p><strong>Settings:</strong> 8 inference steps, 512x512 resolution</p><h3 id="Generation-Tests"><a href="#Generation-Tests" class="headerlink" title="Generation Tests"></a>Generation Tests</h3><p><strong>Test 1: Cold start</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Prompt: &quot;A film photography shot. This item is placed on a wooden desk </span><br><span class="line">         in a cozy study room. Warm afternoon sunlight streams through </span><br><span class="line">         the window.&quot;</span><br><span class="line">Subject: Toy robot figure</span><br><span class="line">Time: 108 seconds</span><br><span class="line">Quality: Excellent, subject preserved with accurate placement</span><br></pre></td></tr></table></figure><p><strong>Test 2: Immediate follow-up</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Prompt: &quot;On Christmas evening, on a crowded sidewalk, this item sits </span><br><span class="line">         covered in snow wearing a Santa hat.&quot;</span><br><span class="line">Subject: Same toy robot</span><br><span class="line">Time: 11 seconds</span><br><span class="line">Quality: Excellent, consistent subject representation</span><br></pre></td></tr></table></figure><p><strong>Test 3: Third generation</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Prompt: &quot;Underwater photography. This item sits on a coral reef with </span><br><span class="line">         tropical fish swimming around it.&quot;</span><br><span class="line">Subject: Same toy robot</span><br><span class="line">Time: 10 seconds</span><br><span class="line">Quality: Good, some water distortion artifacts (expected)</span><br></pre></td></tr></table></figure><h3 id="Resource-Monitoring"><a href="#Resource-Monitoring" class="headerlink" title="Resource Monitoring"></a>Resource Monitoring</h3><p>During active generation:</p><ul><li>GPU Utilization: 95-100%</li><li>VRAM Usage: 10.2GB &#x2F; 12GB (85%)</li><li>System RAM: 52GB &#x2F; 64GB (model pinning)</li><li>CPU Usage: 15-20% (mainly data preprocessing)</li><li>Power Draw: 170W (RTX 3060 TDP)</li></ul><h3 id="Storage-Impact"><a href="#Storage-Impact" class="headerlink" title="Storage Impact"></a>Storage Impact</h3><p>HDD vs SSD comparison (estimated):</p><ul><li><strong>HDD</strong>: 2-3 minutes initial load from disk</li><li><strong>SSD</strong>: 30-45 seconds initial load (2.5x faster)</li><li><strong>During generation</strong>: No difference (models in RAM)</li></ul><p>Recommendation: SSD for faster startup, but not required for generation performance.</p><h2 id="Lessons-Learned-What-Works-and-What-Doesn‚Äôt"><a href="#Lessons-Learned-What-Works-and-What-Doesn‚Äôt" class="headerlink" title="Lessons Learned: What Works and What Doesn‚Äôt"></a>Lessons Learned: What Works and What Doesn‚Äôt</h2><p>After extensive testing and iteration, here are the key insights organized by category. These lessons can save you hours of troubleshooting if you‚Äôre implementing something similar.</p><h3 id="Memory-Management-Insights"><a href="#Memory-Management-Insights" class="headerlink" title="Memory Management Insights"></a>Memory Management Insights</h3><p><strong>‚úÖ Profile 3 is the Sweet Spot for 12GB Cards</strong></p><p>Tested all five mmgp profiles extensively. Profile 3 provides the perfect balance:</p><ul><li>Stable VRAM usage at 85% capacity (10.2GB &#x2F; 12GB)</li><li>Fast inference times (10s per image)</li><li>No OOM errors or crashes across 100+ test generations</li></ul><p>Profiles 1-2 required more VRAM than available, Profiles 4-5 were unnecessarily slow.</p><p><strong>‚úÖ RAM Pinning Eliminates the Disk Bottleneck</strong></p><p>The 75% RAM allocation strategy (48GB pinned) was crucial:</p><ul><li>First load: 2-3 minutes from HDD to RAM (one-time cost)</li><li>Subsequent loads: &lt;5 seconds from pinned RAM to VRAM</li><li>Models persist across generations with zero disk I&#x2F;O</li></ul><p>Without pinning, every generation would require disk access - absolutely impractical.</p><p><strong>‚ö†Ô∏è WSL2 Memory Limits Are Deceiving</strong></p><p>Initial attempts with default WSL2 settings failed. The issue:</p><ul><li>Host system: 64GB RAM available</li><li>WSL2 container: Only sees ~31GB (50% default limit)</li><li>mmgp profile calculation: Incorrectly assumes full RAM available</li></ul><p><strong>Solution:</strong> Explicitly configure <code>.wslconfig</code> to allocate more memory to WSL2, or force mmgp to use <code>perc_reserved_mem_max=0.75</code> parameter.</p><p><strong>‚ùå Auto-Offloading Strategies Don‚Äôt Work Well</strong></p><p>Tried mmgp‚Äôs <code>offloadAfterEveryCall</code> feature - it caused frequent crashes:</p><ul><li>Unpredictable VRAM usage patterns</li><li>Race conditions between loading&#x2F;offloading</li><li>No performance benefit over persistent loading</li></ul><p>Lesson: For sequential generation workloads, keep models loaded.</p><h3 id="Storage-and-I-x2F-O-Optimization"><a href="#Storage-and-I-x2F-O-Optimization" class="headerlink" title="Storage and I&#x2F;O Optimization"></a>Storage and I&#x2F;O Optimization</h3><p><strong>üìä HDD vs SSD Impact Analysis</strong></p><table><thead><tr><th>Phase</th><th>HDD</th><th>SSD</th><th>Impact</th></tr></thead><tbody><tr><td>Initial model download</td><td>5-10 min</td><td>5-10 min</td><td>Network-bound</td></tr><tr><td>First load (disk ‚Üí RAM)</td><td>2-3 min</td><td>30-45 sec</td><td><strong>4x faster</strong></td></tr><tr><td>RAM ‚Üí VRAM transfer</td><td>200ms&#x2F;GB</td><td>200ms&#x2F;GB</td><td>RAM speed</td></tr><tr><td>During generation</td><td>0 disk I&#x2F;O</td><td>0 disk I&#x2F;O</td><td>No difference</td></tr></tbody></table><p><strong>Key Insight:</strong> SSD only matters for startup time. Once models are in RAM, storage speed is irrelevant. If you‚Äôre doing many generations in one session, HDD is perfectly acceptable.</p><p><strong>üíæ Volume Mount Strategy Was Critical</strong></p><p>Storing models on the host filesystem (<code>-v D:\_Models:/app/models</code>) provided:</p><ul><li>Persistence across container rebuilds</li><li>Ability to share models between different containers</li><li>Easy backup and version management</li><li>No re-downloading during development iterations</li></ul><p>Without this, every code change would require re-downloading 35GB of models.</p><h3 id="Configuration-and-Deployment"><a href="#Configuration-and-Deployment" class="headerlink" title="Configuration and Deployment"></a>Configuration and Deployment</h3><p><strong>‚úÖ Gradio Provided Zero-Effort UI</strong></p><p>Using Gradio for the web interface was brilliant:</p><ul><li>20 lines of Python for complete web UI</li><li>Automatic file upload handling</li><li>Built-in image preview and download</li><li>No frontend development required</li></ul><p>Alternative approaches (Flask, React frontend) would have taken days vs hours.</p><p><strong>‚úÖ Docker Isolated the Complexity</strong></p><p>Containerization proved invaluable:</p><ul><li>No conflicts with host Python environment</li><li>Reproducible across machines (tested on 3 different PCs)</li><li>Easy version control of entire stack</li><li>Simple deployment (<code>docker run</code> and done)</li></ul><p><strong>‚ùå Profile 1 Caused Out-of-Memory Errors</strong></p><p>Attempted to use Profile 1 (full model in VRAM) for maximum speed:</p><ul><li>Required 16GB+ VRAM</li><li>RTX 3060‚Äôs 12GB couldn‚Äôt handle it</li><li>Resulted in CUDA OOM errors mid-generation</li></ul><p>Lesson: Always profile your actual available memory, not theoretical specs.</p><h3 id="Quality-and-Performance-Trade-offs"><a href="#Quality-and-Performance-Trade-offs" class="headerlink" title="Quality and Performance Trade-offs"></a>Quality and Performance Trade-offs</h3><p><strong>‚úÖ 8-Bit Quantization Had Minimal Quality Impact</strong></p><p>Side-by-side comparison of T5 encoder outputs:</p><ul><li>FP16 (original): Baseline quality</li><li>INT8 (quantized): &lt;5% subjective quality difference</li><li>Memory savings: 8.8GB ‚Üí 4.4GB (50% reduction)</li></ul><p><strong>Conclusion:</strong> For text encoding tasks, 8-bit quantization is essentially free VRAM.</p><p><strong>üìà Generation Speed Met Targets</strong></p><table><thead><tr><th>Goal</th><th>Result</th><th>Status</th></tr></thead><tbody><tr><td>First gen &lt; 2 min</td><td>110s</td><td>‚úÖ Achieved</td></tr><tr><td>Subsequent gen &lt; 15s</td><td>10s</td><td>‚úÖ Exceeded</td></tr><tr><td>VRAM stable</td><td>10-12GB consistent</td><td>‚úÖ Achieved</td></tr><tr><td>Quality acceptable</td><td>Excellent outputs</td><td>‚úÖ Achieved</td></tr></tbody></table><p>The 10-second generation time makes this practical for real creative work.</p><h2 id="Production-Considerations"><a href="#Production-Considerations" class="headerlink" title="Production Considerations"></a>Production Considerations</h2><h3 id="For-Personal-Use"><a href="#For-Personal-Use" class="headerlink" title="For Personal Use"></a>For Personal Use</h3><p>This setup works great for:</p><ul><li>Hobbyist AI experimentation</li><li>Content creation (social media, art projects)</li><li>Proof-of-concept development</li><li>Learning FLUX.1 architecture</li></ul><h3 id="For-Commercial-Use"><a href="#For-Commercial-Use" class="headerlink" title="For Commercial Use"></a>For Commercial Use</h3><p>Consider these factors:</p><ul><li>Generation time: 10s&#x2F;image √ó 1000 images &#x3D; 2.8 hours</li><li>Scalability: Single GPU, no batch processing</li><li>Reliability: Consumer GPU thermal throttling under sustained load</li><li>Support: mmgp is community-maintained, not enterprise-supported</li></ul><p>For production workloads, consider:</p><ul><li>Cloud GPUs (Azure N-Series VMs or Azure Container Apps with GPU nodes) - minimum A40&#x2F;A100</li><li>Local GPU upgrade to RTX 4090 or A6000</li><li>Batch processing optimizations</li><li>Multiple parallel containers</li></ul><h2 id="Docker-Hub-amp-Repository"><a href="#Docker-Hub-amp-Repository" class="headerlink" title="Docker Hub &amp; Repository"></a>Docker Hub &amp; Repository</h2><p>The complete implementation is available:</p><ul><li>GitHub: <a target="_blank" rel="noopener" href="https://github.com/Ricky-G/docker-ai-models/tree/main/omnicontrol">docker-ai-models&#x2F;omnicontrol</a></li><li>README: Full setup instructions and troubleshooting</li><li>Dockerfile: Production-ready container definition</li><li>Source code: Custom FLUX integration with mmgp</li></ul><h3 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Clone repository</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Ricky-G/docker-ai-models.git</span><br><span class="line"><span class="built_in">cd</span> docker-ai-models/omnicontrol</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build container</span></span><br><span class="line">docker build -t omnicontrol .</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run with HuggingFace token</span></span><br><span class="line">docker run -d --gpus all --name omnicontrol \</span><br><span class="line">  -p 7860:7860 \</span><br><span class="line">  -v D:\_Models\omnicontrol:/app/models \</span><br><span class="line">  -e HF_TOKEN=your_token_here \</span><br><span class="line">  omnicontrol</span><br><span class="line"></span><br><span class="line"><span class="comment"># Access web interface</span></span><br><span class="line"><span class="comment"># http://localhost:7860</span></span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Running FLUX.1 OmniControl on a 12GB RTX 3060 is not only possible but practical. Through careful memory management with mmgp, strategic quantization, and container optimization, we achieved:</p><ul><li>‚úÖ 10-second generation times (after initial load)</li><li>‚úÖ Stable VRAM usage across multiple generations</li><li>‚úÖ No quality degradation from quantization</li><li>‚úÖ Reproducible Docker deployment</li></ul><p>The key insight: <strong>Memory management is more important than raw VRAM capacity</strong>. With the right tools and configuration, consumer GPUs can run models designed for datacenter hardware.</p><p>If you have an RTX 3060 (or similar 12GB card) collecting dust because you thought it couldn‚Äôt handle modern AI models, give this approach a try. The democratization of AI isn‚Äôt just about open-source models - it‚Äôs about making them runnable on hardware people actually own.</p><hr><p><strong>Hardware tested:</strong> RTX 3060 12GB, 64GB RAM, Windows 11 + WSL2</p><p><strong>Software stack:</strong> Docker Desktop, NVIDIA Container Toolkit, mmgp 3.6.9</p><p><strong>Model:</strong> FLUX.1-schnell + OminiControl LoRA</p><p><strong>Performance:</strong> 10s per 512x512 image (8 steps)</p><hr><p><strong>References:</strong></p><ul><li>Memory optimization via <a target="_blank" rel="noopener" href="https://pypi.org/project/mmgp/">mmgp</a> (Memory Management for GPU Poor)</li><li><a target="_blank" rel="noopener" href="https://huggingface.co/black-forest-labs/FLUX.1-schnell">FLUX.1-schnell Model</a></li><li><a target="_blank" rel="noopener" href="https://huggingface.co/Yuanshi/OmniControl">OminiControl LoRA</a></li><li><a target="_blank" rel="noopener" href="https://github.com/Ricky-G/docker-ai-models/tree/main/omnicontrol">Docker Implementation Repository</a></li></ul><hr><p><strong>Image Credits:</strong></p><ul><li>Main image generated by <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/dall-e?view=foundry-classic&tabs=gpt-image-1">GPT-Image-1.5</a></li></ul></div><div class="article-licensing box"><div class="licensing-title"><p>Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060</p><p><a href="https://clouddev.blog/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/">https://clouddev.blog/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Ricky Gummadi</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-11-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2026-01-07</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/LLMs/">LLMs</a><a class="link-muted mr-2" rel="tag" href="/tags/Local-GPUs/">Local GPUs</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=62df1ab6e0f8f90019cf2f78&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="https://www.buymeacoffee.com/rickygummaT" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Azure/AI/Networking/microsoft-foundry-cross-region-with-private-endpoints-part-1/"><span class="level-item">Microsoft Foundry Cross-Region with Private Endpoints (Part 1)</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config=function(){this.page.url="https://clouddev.blog/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/",this.page.identifier="AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/"};!function(){var e=document,t=e.createElement("script");t.src="//clouddev.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}()</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar_2.jpg" alt="Ricky Gummadi"></figure><p class="title is-size-4 is-block" style="line-height:inherit">Ricky Gummadi</p><p class="is-size-6 is-block">Cloud Solution Architect</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Auckland, New Zealand</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">67</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Ricky-G" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Ricky-G"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="StackOverflow" href="https://stackoverflow.com/users/441914/ricky-gummadi?tab=profile"><i class="fab fa-stack-overflow"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Linkedin" href="https://www.linkedin.com/in/rickygummadi/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#What-is-FLUX-1-OmniControl"><span class="level-left"><span class="level-item">1</span><span class="level-item">What is FLUX.1 OmniControl?</span></span></a></li><li><a class="level is-mobile" href="#The-Challenge-Model-Size-vs-Available-VRAM"><span class="level-left"><span class="level-item">2</span><span class="level-item">The Challenge: Model Size vs Available VRAM</span></span></a></li><li><a class="level is-mobile" href="#Understanding-mmgp-Memory-Management-for-GPU-Poor"><span class="level-left"><span class="level-item">3</span><span class="level-item">Understanding mmgp: Memory Management for GPU Poor</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#8-Bit-Quantization"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">8-Bit Quantization</span></span></a></li><li><a class="level is-mobile" href="#Dynamic-VRAM-x2F-RAM-Offloading"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Dynamic VRAM/RAM Offloading</span></span></a></li><li><a class="level is-mobile" href="#RAM-Pinning-Strategy"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">RAM Pinning Strategy</span></span></a></li><li><a class="level is-mobile" href="#Profile-System"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">Profile System</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Prerequisites-What-You‚Äôll-Need"><span class="level-left"><span class="level-item">4</span><span class="level-item">Prerequisites: What You‚Äôll Need</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Hardware-Requirements"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Hardware Requirements</span></span></a></li><li><a class="level is-mobile" href="#Software-Requirements"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Software Requirements</span></span></a></li><li><a class="level is-mobile" href="#Account-Requirements"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">Account Requirements</span></span></a></li><li><a class="level is-mobile" href="#Verification-Steps"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">Verification Steps</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Docker-Architecture-Why-Containerization"><span class="level-left"><span class="level-item">5</span><span class="level-item">Docker Architecture: Why Containerization?</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Container-Structure"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Container Structure</span></span></a></li><li><a class="level is-mobile" href="#Volume-Mounts"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Volume Mounts</span></span></a></li><li><a class="level is-mobile" href="#GPU-Access"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">GPU Access</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Implementation-Details-From-Code-to-Running-System"><span class="level-left"><span class="level-item">6</span><span class="level-item">Implementation Details: From Code to Running System</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Startup-Sequence"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">Startup Sequence</span></span></a></li><li><a class="level is-mobile" href="#Generation-Performance"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">Generation Performance</span></span></a></li><li><a class="level is-mobile" href="#Critical-Fix-Preventing-VRAM-Clearing"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">Critical Fix: Preventing VRAM Clearing</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Real-World-Testing-Results"><span class="level-left"><span class="level-item">7</span><span class="level-item">Real-World Testing Results</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Test-Configuration"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Test Configuration</span></span></a></li><li><a class="level is-mobile" href="#Generation-Tests"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Generation Tests</span></span></a></li><li><a class="level is-mobile" href="#Resource-Monitoring"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">Resource Monitoring</span></span></a></li><li><a class="level is-mobile" href="#Storage-Impact"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">Storage Impact</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Lessons-Learned-What-Works-and-What-Doesn‚Äôt"><span class="level-left"><span class="level-item">8</span><span class="level-item">Lessons Learned: What Works and What Doesn‚Äôt</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Memory-Management-Insights"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Memory Management Insights</span></span></a></li><li><a class="level is-mobile" href="#Storage-and-I-x2F-O-Optimization"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">Storage and I/O Optimization</span></span></a></li><li><a class="level is-mobile" href="#Configuration-and-Deployment"><span class="level-left"><span class="level-item">8.3</span><span class="level-item">Configuration and Deployment</span></span></a></li><li><a class="level is-mobile" href="#Quality-and-Performance-Trade-offs"><span class="level-left"><span class="level-item">8.4</span><span class="level-item">Quality and Performance Trade-offs</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Production-Considerations"><span class="level-left"><span class="level-item">9</span><span class="level-item">Production Considerations</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#For-Personal-Use"><span class="level-left"><span class="level-item">9.1</span><span class="level-item">For Personal Use</span></span></a></li><li><a class="level is-mobile" href="#For-Commercial-Use"><span class="level-left"><span class="level-item">9.2</span><span class="level-item">For Commercial Use</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Docker-Hub-amp-Repository"><span class="level-left"><span class="level-item">10</span><span class="level-item">Docker Hub &amp; Repository</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Quick-Start"><span class="level-left"><span class="level-item">10.1</span><span class="level-item">Quick Start</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">11</span><span class="level-item">Conclusion</span></span></a></li></ul></div></div><style>#toc .menu-list>li>a.is-active+.menu-list{display:block}#toc .menu-list>li>a+.menu-list{display:none}</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/LLMs/"><span class="level-start"><span class="level-item">LLMs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/AKS/"><span class="level-start"><span class="level-item">AKS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/AKS/AGIC/"><span class="level-start"><span class="level-item">AGIC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Azure/"><span class="level-start"><span class="level-item">Azure</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/Azure/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Azure/AI/Networking/"><span class="level-start"><span class="level-item">Networking</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Azure/AI/Speech/"><span class="level-start"><span class="level-item">Speech</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Azure/AI/Voice-Live-API/"><span class="level-start"><span class="level-item">Voice Live API</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Azure/Container-Apps/"><span class="level-start"><span class="level-item">Container Apps</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Azure/DevOps/"><span class="level-start"><span class="level-item">DevOps</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Azure/DevOps/Development/"><span class="level-start"><span class="level-item">Development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Azure/DevTest-Labs/"><span class="level-start"><span class="level-item">DevTest Labs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Azure/Function-Apps/"><span class="level-start"><span class="level-item">Function Apps</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Azure/Function-Apps/NET/"><span class="level-start"><span class="level-item">.NET</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Azure/Function-Apps/Security/"><span class="level-start"><span class="level-item">Security</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Azure/Logic-Apps/"><span class="level-start"><span class="level-item">Logic Apps</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Azure/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Azure/Storage/Azurite/"><span class="level-start"><span class="level-item">Azurite</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Azure-DevOps/"><span class="level-start"><span class="level-item">Azure DevOps</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Azure-DevOps/Azure-DevOps-API/"><span class="level-start"><span class="level-item">Azure DevOps API</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Blog/"><span class="level-start"><span class="level-item">Blog</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Engineering/"><span class="level-start"><span class="level-item">Engineering</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Engineering/Tooling/"><span class="level-start"><span class="level-item">Tooling</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/GitHub/"><span class="level-start"><span class="level-item">GitHub</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/GitHub/Actions/"><span class="level-start"><span class="level-item">Actions</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/11/"><span class="level-start"><span class="level-item">November 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/10/"><span class="level-start"><span class="level-item">October 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/09/"><span class="level-start"><span class="level-item">September 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/08/"><span class="level-start"><span class="level-item">August 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/07/"><span class="level-start"><span class="level-item">July 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/06/"><span class="level-start"><span class="level-item">June 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">May 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">April 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/Njl3U2NMelcvYkNDZ2V0elNmQzB6NFpsdXRrT214U09hUTVxeURKSjNUNkFYd2JHWDZMYkcwclJuSWdTMDlLdmNNbEtSS0pZbmxtM2M2UDFjemErOWlSZkxoUjlnNExDd3BmZlIzUUFBd2tOREZZTURRTVNyc1Z4MHR5MWJ4OW18VmpXd085SlVEMWZZVHd5UlJCZXkyMTJqOENaRHlzMGVmUGkxek1iemNuWT0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div><p class="help">Please enter your email address to get updates when new articles are published</p></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/"><img src="/img/running-llms-consumer-gpu.png" alt="Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060"></a></figure><div class="media-content"><p class="date"><time datetime="2025-11-10T11:00:00.000Z">2025-11-11</time></p><p class="title"><a href="/AI/LLMs/running-flux-1-omnicontrol-on-a-consumer-gpu-a-docker-implementation-tested-on-rtx-3060/">Running FLUX.1 OmniControl on a Consumer GPU: A Docker Implementation tested on RTX 3060</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/LLMs/">LLMs</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Azure/AI/Networking/microsoft-foundry-cross-region-with-private-endpoints-part-1/"><img src="/img/foundry-cross-region.png" alt="Microsoft Foundry Cross-Region with Private Endpoints (Part 1)"></a></figure><div class="media-content"><p class="date"><time datetime="2025-10-10T11:00:00.000Z">2025-10-11</time></p><p class="title"><a href="/Azure/AI/Networking/microsoft-foundry-cross-region-with-private-endpoints-part-1/">Microsoft Foundry Cross-Region with Private Endpoints (Part 1)</a></p><p class="categories"><a href="/categories/Azure/">Azure</a> / <a href="/categories/Azure/AI/">AI</a> / <a href="/categories/Azure/AI/Networking/">Networking</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Engineering/Tooling/pimp-my-terminal-terminal-customization-with-oh-my-posh-a-cloud-native-terminal-setup/"><img src="/img/terminal-pimp.png" alt="Pimp My Terminal - Terminal Customization with Oh My Posh - A Cloud Native Terminal Setup"></a></figure><div class="media-content"><p class="date"><time datetime="2025-09-19T12:00:00.000Z">2025-09-20</time></p><p class="title"><a href="/Engineering/Tooling/pimp-my-terminal-terminal-customization-with-oh-my-posh-a-cloud-native-terminal-setup/">Pimp My Terminal - Terminal Customization with Oh My Posh - A Cloud Native Terminal Setup</a></p><p class="categories"><a href="/categories/Engineering/">Engineering</a> / <a href="/categories/Engineering/Tooling/">Tooling</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Azure-DevOps/Azure-DevOps-API/automating-searchable-branch-configuration-in-azure-devops-repos-via-rest-api/"><img src="/img/automate-branch-ado-api.png" alt="Automating Searchable Branch Configuration in Azure DevOps Repos via REST API"></a></figure><div class="media-content"><p class="date"><time datetime="2025-08-14T12:00:00.000Z">2025-08-15</time></p><p class="title"><a href="/Azure-DevOps/Azure-DevOps-API/automating-searchable-branch-configuration-in-azure-devops-repos-via-rest-api/">Automating Searchable Branch Configuration in Azure DevOps Repos via REST API</a></p><p class="categories"><a href="/categories/Azure-DevOps/">Azure DevOps</a> / <a href="/categories/Azure-DevOps/Azure-DevOps-API/">Azure DevOps API</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/Azure/AI/Voice-Live-API/building-voice-agents-with-azure-communication-services-voice-live-api-and-azure-ai-agent-service/"><img src="/img/building-voice-agents.png" alt="Building Voice Agents with Azure Communication Services Voice Live API and Azure AI Agent Service"></a></figure><div class="media-content"><p class="date"><time datetime="2025-07-07T12:00:00.000Z">2025-07-08</time></p><p class="title"><a href="/Azure/AI/Voice-Live-API/building-voice-agents-with-azure-communication-services-voice-live-api-and-azure-ai-agent-service/">Building Voice Agents with Azure Communication Services Voice Live API and Azure AI Agent Service</a></p><p class="categories"><a href="/categories/Azure/">Azure</a> / <a href="/categories/Azure/AI/">AI</a> / <a href="/categories/Azure/AI/Voice-Live-API/">Voice Live API</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1649927746910796" data-ad-slot="7373409515" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/NET/"><span class="tag">.NET</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AGIC/"><span class="tag">AGIC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI-Development/"><span class="tag">AI Development</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI-Voice-Agents/"><span class="tag">AI Voice Agents</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AKS/"><span class="tag">AKS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Application-Gateway/"><span class="tag">Application Gateway</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoGen/"><span class="tag">AutoGen</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure/"><span class="tag">Azure</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-AI-Agent-Service/"><span class="tag">Azure AI Agent Service</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-App-Service/"><span class="tag">Azure App Service</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-Blob-Storage/"><span class="tag">Azure Blob Storage</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-CLI/"><span class="tag">Azure CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-Communication-Services/"><span class="tag">Azure Communication Services</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-Container-Apps/"><span class="tag">Azure Container Apps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-Dev-Test-Labs/"><span class="tag">Azure Dev Test Labs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-DevOps/"><span class="tag">Azure DevOps</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-Logic-Apps/"><span class="tag">Azure Logic Apps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azure-Policy/"><span class="tag">Azure Policy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azurite/"><span class="tag">Azurite</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Blog/"><span class="tag">Blog</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CI-CD/"><span class="tag">CI/CD</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Code-Search/"><span class="tag">Code Search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Collaboration/"><span class="tag">Collaboration</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Contact-Center/"><span class="tag">Contact Center</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Containers/"><span class="tag">Containers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dev-Containers/"><span class="tag">Dev Containers</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DevOps/"><span class="tag">DevOps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Developer-Environments/"><span class="tag">Developer Environments</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Development-Environment/"><span class="tag">Development Environment</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DotNet/"><span class="tag">DotNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function-Apps/"><span class="tag">Function Apps</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GZip/"><span class="tag">GZip</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GitHub/"><span class="tag">GitHub</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IP-Restrictions/"><span class="tag">IP Restrictions</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ingress/"><span class="tag">Ingress</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kubernetes/"><span class="tag">Kubernetes</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLMs/"><span class="tag">LLMs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Local-GPUs/"><span class="tag">Local GPUs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Logic-Apps/"><span class="tag">Logic Apps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Microsoft-Foundry/"><span class="tag">Microsoft Foundry</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Networking/"><span class="tag">Networking</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenAI/"><span class="tag">OpenAI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Personal/"><span class="tag">Personal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Policy-Configuration/"><span class="tag">Policy Configuration</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PowerShell/"><span class="tag">PowerShell</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Private-Endpoints/"><span class="tag">Private Endpoints</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Private-Link/"><span class="tag">Private Link</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/REST-API/"><span class="tag">REST API</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Realtime/"><span class="tag">Realtime</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Remote-Development/"><span class="tag">Remote Development</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Security/"><span class="tag">Security</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Serverless/"><span class="tag">Serverless</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Single-Threaded-Apps/"><span class="tag">Single Threaded Apps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Speech-Services/"><span class="tag">Speech Services</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Storage/"><span class="tag">Storage</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TFS/"><span class="tag">TFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TFVC/"><span class="tag">TFVC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tar/"><span class="tag">Tar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VS-Code/"><span class="tag">VS Code</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Virtual-Machines/"><span class="tag">Virtual Machines</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Voice-Live-API/"><span class="tag">Voice Live API</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebRTC/"><span class="tag">WebRTC</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Exploring Azure, DevOps and Software Development" height="28"></a><p class="is-size-7"><span>&copy; 2026 &nbsp; | &nbsp; Ricky Gummadi &nbsp; | &nbsp;</span><span class="icon-animate"><i class="fa fa-heart"></i></span><br>All views expressed here are strictly and entirely my own.<br>~ ¬†Powered by<a href="https://hexo.io/" target="_blank" rel="noopener">¬† Hexo</a> &amp;<a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">¬† Icarus</a> &amp;<a href="https://bulma.io/" target="_blank" rel="noopener">¬† Bulma</a> &amp;<a href="https://pages.github.com/" target="_blank" rel="noopener">¬† GitHub Pages</a><br></p><div><script src="https://unpkg.com/mermaid@11.8.0/dist/mermaid.min.js"></script><script>document.addEventListener("DOMContentLoaded",(function(){window.mermaid&&mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#ffffff",primaryTextColor:"#333333",primaryBorderColor:"#4a90e2",lineColor:"#333333",secondaryColor:"#f8f9fa",tertiaryColor:"#ffffff",background:"#ffffff",mainBkg:"#ffffff",secondBkg:"#f8f9fa",tertiaryBkg:"#ffffff"},flowchart:{htmlLabels:!0,curve:"basis",padding:20,nodeSpacing:100,rankSpacing:80,useMaxWidth:!0,diagramPadding:20},maxWidth:"100%"})}))</script></div><p></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Github" href="https://github.com/Ricky-G"><i class="fab fa-github"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="StackOverflow" href="https://stackoverflow.com/users/441914/ricky-gummadi?tab=profile"><i class="fab fa-stack-overflow"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Linkedin" href="https://www.linkedin.com/in/rickygummadi/"><i class="fab fa-linkedin"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load",()=>{window.cookieconsent.initialise({type:"opt-in",theme:"edgeless",static:!1,position:"bottom-left",content:{message:"This website uses cookies to improve your experience.",dismiss:"Got it!",allow:"Allow cookies",deny:"Decline",link:"Learn more",policy:"Cookie Policy",href:"/cookies/"},palette:{popup:{background:"#edeff5",text:"#838391"},button:{background:"#4b81e8"}}})})</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load",()=>{"function"==typeof $.fn.lightGallery&&$(".article").lightGallery({selector:".gallery-item"}),"function"==typeof $.fn.justifiedGallery&&($(".justified-gallery > p > .gallery-item").length&&$(".justified-gallery > p > .gallery-item").unwrap(),$(".justified-gallery").justifiedGallery())})</script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">√ó</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",(function(){loadInsight({contentUrl:"/content.json"},{hint:"Type something...",untitled:"(Untitled)",posts:"Posts",pages:"Pages",categories:"Categories",tags:"Tags"})}))</script>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>
<style>
.geDiagramContainer { width: 100% !important; }
</style>
</body>
</html>